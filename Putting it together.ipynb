{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca7508a-ea17-464a-865e-2517f0f6a386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 16 14:35:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.01              Driver Version: 536.67       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070        On  | 00000000:24:00.0  On |                  N/A |\n",
      "|  0%   37C    P8              10W / 200W |   6210MiB / 12282MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A       147      C   /ollama                                   N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f4b8c8-773d-4858-b755-c9a35804c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir soundbites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67da8c03-8d03-4eb2-9a30-50062edf7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44dd5966-f1d5-40f2-a271-42e0affe2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import IPython\n",
    "\n",
    "from TTS.api import TTS\n",
    "from pprint import pprint\n",
    "from IPython.display import Audio\n",
    "from langchain_community.llms import Ollama\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.base import BaseCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1426e32a-b929-4636-87d1-164c5acf1055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "pygame.mixer.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b411545-db93-4cc7-a069-6284f7fb35c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/vctk/vits is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > initialization of speaker-embedding layers.\n"
     ]
    }
   ],
   "source": [
    "tts = TTS(\"tts_models/en/vctk/vits\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f650bbe-8079-46dc-818a-2308fe4729dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav = tts.tts(text=\"I'm just an AI, I don't have feelings or emotions like humans do, so I can't experience the world in the same way that you do. However, I'm here to help answer any questions you may have, provide information, and assist you in any way I can. Is there something specific you would like to know or discuss?\", speaker=tts.speakers[2]) #, speaker_wav=\"audio/scarlett_johanson.wav\", language=\"en\")\n",
    "# Audio(np.array(wav), rate=20000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3fe10-4882-45c5-8541-8f020a53c8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5787d671-55ce-487b-a59b-ce6cfd102bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama( model=\"openhermes\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d81f4c-091f-438b-b83e-3e44cf4e63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamingCallback(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.content = \"\"\n",
    "        \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.content += token\n",
    "        if self.content.strip()[-1] in ['.', '?', '!', ':']:\n",
    "            wav = tts.tts(self.content, speaker=tts.speakers[2], verbose=False)\n",
    "            IPython.display.display(Audio(np.array(wav), rate=20000))\n",
    "            print(self.content)\n",
    "            self.content = \"\"\n",
    "            sleep(len(wav) / 20000)\n",
    "\n",
    "handler = MyStreamingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59b37d2d-108e-4633-9201-cc383b67844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamingCallback(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.content = \"\"\n",
    "        \n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.content += token\n",
    "        if self.content.strip()[-1] in ['.', '?', '!', ':']:\n",
    "            wav = tts.tts(self.content, speaker=tts.speakers[2], verbose=False)\n",
    "            filename = \"soundbites/\"+ self.content[:12] + \".wav\"\n",
    "            tts.tts_to_file(text=self.content, speaker=tts.speakers[2], file_path=filename)\n",
    "            \n",
    "            if pygame.mixer.music.get_busy() == True:\n",
    "                pygame.mixer.music.queue(filename)\n",
    "            else:\n",
    "                pygame.mixer.music.load(filename)\n",
    "                pygame.mixer.music.play()\n",
    "            print(self.content)\n",
    "            self.content = \"\"\n",
    "\n",
    "handler = MyStreamingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eecd59db-174b-46ef-9dd9-8535b9412fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"soundbites/ However, I'.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661039a0-5baa-4120-9333-0866b8e23dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pygame.mixer.music.get_busy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28d98dab-71ef-4cef-bfc1-10a76943bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.music.load(filename)\n",
    "pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8b56b42-7276-423d-a51b-7bb63a581f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = Ollama(\n",
    "    model=\"openhermes\",\n",
    "    callbacks=[handler],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89ee7369-5fa7-4bbd-862e-435b9ef019e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Hello!']\n",
      " > Processing time: 1.0045979022979736\n",
      " > Real-time factor: 0.9199079628600632\n",
      " > Text splitted to sentences.\n",
      "['Hello!']\n",
      " > Processing time: 0.0602872371673584\n",
      " > Real-time factor: 0.05520488287127296\n",
      "Hello!\n",
      " > Text splitted to sentences.\n",
      "[\"I'm a language model, so I don't have feelings like humans.\"]\n",
      " > Processing time: 0.2721138000488281\n",
      " > Real-time factor: 0.08309479962160232\n",
      " > Text splitted to sentences.\n",
      "[\"I'm a language model, so I don't have feelings like humans.\"]\n",
      " > Processing time: 0.4278907775878906\n",
      " > Real-time factor: 0.13497069761119518\n",
      " I'm a language model, so I don't have feelings like humans.\n",
      " > Text splitted to sentences.\n",
      "[\"But I'm always ready to help and assist in any way that I can!\"]\n",
      " > Processing time: 0.14227533340454102\n",
      " > Real-time factor: 0.04571536345258407\n",
      " > Text splitted to sentences.\n",
      "[\"But I'm always ready to help and assist in any way that I can!\"]\n",
      " > Processing time: 0.14107084274291992\n",
      " > Real-time factor: 0.04566907567655309\n",
      " But I'm always ready to help and assist in any way that I can!\n",
      " > Text splitted to sentences.\n",
      "['How about you, how can I assist you today?']\n",
      " > Processing time: 0.14040040969848633\n",
      " > Real-time factor: 0.04995851139058262\n",
      " > Text splitted to sentences.\n",
      "['How about you, how can I assist you today?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in MyStreamingCallback.on_llm_new_token callback: IndexError('string index out of range')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 0.152923583984375\n",
      " > Real-time factor: 0.06361237977013788\n",
      " How about you, how can I assist you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm a language model, so I don't have feelings like humans. But I'm always ready to help and assist in any way that I can! How about you, how can I assist you today?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"Hey how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a248ff78-8898-4de4-a3b9-9a5da6543504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm(\"How are you feeling today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "322f89bd-126e-429a-a161-49b9f2ea589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "whisper_model_id = \"distil-whisper/distil-medium.en\"\n",
    "\n",
    "whisper = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    whisper_model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True# , use_flash_attention_2=True\n",
    ")\n",
    "whisper.to(\"cuda\")\n",
    "# model = model.to_bettertransformer() # we are using optimum BetterTransformer since Flash Attention 2 isn't supported on Colab\n",
    "processor = AutoProcessor.from_pretrained(whisper_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1ad958f-a079-4329-a4dc-a6cee5917854",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=whisper,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=15, #long form transcription\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch.float16,\n",
    "    device='cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f52c9735-db35-421f-937f-7f9072c3d649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismail/envs/hym/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Hi Ismail!']\n",
      " > Processing time: 0.3086822032928467\n",
      " > Real-time factor: 0.2554970939417143\n",
      " > Text splitted to sentences.\n",
      "['Hi Ismail!']\n",
      " > Processing time: 0.13989591598510742\n",
      " > Real-time factor: 0.11469010066447125\n",
      " Hi Ismail!\n",
      " > Text splitted to sentences.\n",
      "[\"I'm good, thank you for asking.\"]\n",
      " > Processing time: 0.17240381240844727\n",
      " > Real-time factor: 0.09277391799117196\n",
      " > Text splitted to sentences.\n",
      "[\"I'm good, thank you for asking.\"]\n",
      " > Processing time: 0.15323686599731445\n",
      " > Real-time factor: 0.08297821451966561\n",
      " I'm good, thank you for asking.\n",
      " > Text splitted to sentences.\n",
      "['How about you?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in MyStreamingCallback.on_llm_new_token callback: IndexError('string index out of range')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 0.14174604415893555\n",
      " > Real-time factor: 0.1119448522100476\n",
      " > Text splitted to sentences.\n",
      "['How about you?']\n",
      " > Processing time: 0.05566596984863281\n",
      " > Real-time factor: 0.046074873692280534\n",
      " How about you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_4261/3537659215.py\", line 24, in transcribe_streaming\n",
      "    content += transcribed_stream\n",
      "UnboundLocalError: local variable 'content' referenced before assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Hello!']\n",
      " > Processing time: 0.16615629196166992\n",
      " > Real-time factor: 0.14898122307070683\n",
      " > Text splitted to sentences.\n",
      "['Hello!']\n",
      " > Processing time: 0.15089988708496094\n",
      " > Real-time factor: 0.14118052063065972\n",
      "Hello!\n",
      " > Text splitted to sentences.\n",
      "[\"It's great to meet you, Ismail.\"]\n",
      " > Processing time: 0.18310117721557617\n",
      " > Real-time factor: 0.09219448660950527\n",
      " > Text splitted to sentences.\n",
      "[\"It's great to meet you, Ismail.\"]\n",
      " > Processing time: 0.14808320999145508\n",
      " > Real-time factor: 0.07327726167665136\n",
      " It's great to meet you, Ismail.\n",
      " > Text splitted to sentences.\n",
      "[\"I'm doing well.\"]\n",
      " > Processing time: 0.05040478706359863\n",
      " > Real-time factor: 0.03980750554270594\n",
      " > Text splitted to sentences.\n",
      "[\"I'm doing well.\"]\n",
      " > Processing time: 0.16103720664978027\n",
      " > Real-time factor: 0.11848873487145138\n",
      " I'm doing well.\n",
      " > Text splitted to sentences.\n",
      "['How about yourself?']\n",
      " > Processing time: 0.14168524742126465\n",
      " > Real-time factor: 0.09455689181715755\n",
      " > Text splitted to sentences.\n",
      "['How about yourself?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in MyStreamingCallback.on_llm_new_token callback: IndexError('string index out of range')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 0.12645316123962402\n",
      " > Real-time factor: 0.08571975545172497\n",
      " How about yourself?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_4261/3537659215.py\", line 24, in transcribe_streaming\n",
      "    content += transcribed_stream\n",
      "UnboundLocalError: local variable 'content' referenced before assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Hi Ismail!']\n",
      " > Processing time: 0.3078947067260742\n",
      " > Real-time factor: 0.24770425727196205\n",
      " > Text splitted to sentences.\n",
      "['Hi Ismail!']\n",
      " > Processing time: 0.14389514923095703\n",
      " > Real-time factor: 0.11159566828019846\n",
      "Hi Ismail!\n",
      " > Text splitted to sentences.\n",
      "[\"It's great to meet you here.\"]\n",
      " > Processing time: 0.14613628387451172\n",
      " > Real-time factor: 0.08387924457082943\n",
      " > Text splitted to sentences.\n",
      "[\"It's great to meet you here.\"]\n",
      " > Processing time: 0.14811325073242188\n",
      " > Real-time factor: 0.08794423682275695\n",
      " It's great to meet you here.\n",
      " > Text splitted to sentences.\n",
      "['How can I help you today?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in MyStreamingCallback.on_llm_new_token callback: IndexError('string index out of range')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 0.15747880935668945\n",
      " > Real-time factor: 0.10117738188563527\n",
      " > Text splitted to sentences.\n",
      "['How can I help you today?']\n",
      " > Processing time: 0.14498567581176758\n",
      " > Real-time factor: 0.09246107564927912\n",
      " How can I help you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_4261/3537659215.py\", line 24, in transcribe_streaming\n",
      "    content += transcribed_stream\n",
      "UnboundLocalError: local variable 'content' referenced before assignment\n",
      "/home/ismail/envs/hym/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Hello Ismail!']\n",
      " > Processing time: 0.24198365211486816\n",
      " > Real-time factor: 0.1780479020666325\n",
      " > Text splitted to sentences.\n",
      "['Hello Ismail!']\n",
      " > Processing time: 0.15158772468566895\n",
      " > Real-time factor: 0.11862966103488787\n",
      " Hello Ismail!\n",
      " > Text splitted to sentences.\n",
      "[\"It's great to meet you.\"]\n",
      " > Processing time: 0.22179317474365234\n",
      " > Real-time factor: 0.14575999949623075\n",
      " > Text splitted to sentences.\n",
      "[\"It's great to meet you.\"]\n",
      " > Processing time: 0.1651601791381836\n",
      " > Real-time factor: 0.10690999148652384\n",
      " It's great to meet you.\n",
      " > Text splitted to sentences.\n",
      "[\"I am an AI language model, so I don't have feelings like humans do, but I am always happy to interact with people and help them in any way I can.\"]\n",
      " > Processing time: 0.18164777755737305\n",
      " > Real-time factor: 0.025314323333628753\n",
      " > Text splitted to sentences.\n",
      "[\"I am an AI language model, so I don't have feelings like humans do, but I am always happy to interact with people and help them in any way I can.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in MyStreamingCallback.on_llm_new_token callback: IndexError('string index out of range')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 0.15716242790222168\n",
      " > Real-time factor: 0.022080688240671758\n",
      " I am an AI language model, so I don't have feelings like humans do, but I am always happy to interact with people and help them in any way I can.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ismail/envs/hym/lib/python3.10/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_4261/3537659215.py\", line 24, in transcribe_streaming\n",
      "    content += transcribed_stream\n",
      "UnboundLocalError: local variable 'content' referenced before assignment\n",
      "/home/ismail/envs/hym/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "def transcribe(filepath):\n",
    "    output = asr_pipe(\n",
    "        filepath,\n",
    "    )\n",
    "    return output[\"text\"]\n",
    "\n",
    "\n",
    "def transcribe_streaming(stream, new_chunk):\n",
    "    sr, y = new_chunk\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    if stream is not None:\n",
    "        stream = np.concatenate([stream, y])\n",
    "    else:\n",
    "        stream = y\n",
    "\n",
    "    transcribed_stream = asr_pipe({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
    "    if transcribed_stream.strip().endswith('?') and len(transcribed_stream) > 30:\n",
    "        for w in llm2(transcribed_stream):\n",
    "            content += transcribed_stream\n",
    "            wav = tts.tts(content, speaker=tts.speakers[2], verbose=False)\n",
    "            filename = \"soundbites/\"+ self.content[:12] + \".wav\"\n",
    "            tts.tts_to_file(text=self.content, speaker=tts.speakers[2], file_path=filename)\n",
    "            print(\"We're here\")\n",
    "            \n",
    "            if pygame.mixer.music.get_busy() == True:\n",
    "                pygame.mixer.music.queue(filename)\n",
    "            else:\n",
    "                pygame.mixer.music.load(filename)\n",
    "                pygame.mixer.music.play()\n",
    "                print(\"Playing\", filename)\n",
    "            print(content)\n",
    "            content = \"\"\n",
    "    \n",
    "    return stream, transcribed_stream\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "mic_transcribe = gr.Interface(\n",
    "    title='My Audio Transcription App Powered by Distill Whisper',\n",
    "    description=\"Start recording\",\n",
    "    fn=transcribe_streaming,\n",
    "    inputs=[\"state\", gr.Audio(sources=\"microphone\", streaming=True)],\n",
    "    outputs=[\"state\", \"text\"],\n",
    "    live=True,\n",
    ")\n",
    "\n",
    "\n",
    "file_transcribe = gr.Interface(\n",
    "    title='My Audio Transcription App Powered by Distill Whisper',\n",
    "    description=\"Upload an audio file\",\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(sources=\"upload\", type=\"filepath\"),\n",
    "    outputs=gr.Textbox(),\n",
    ")\n",
    "\n",
    "\n",
    "gr.close_all()\n",
    "\n",
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe, file_transcribe],\n",
    "        [\"Transcribe Microphone\",  \"Transcribe Audio File\", ],\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b1e72f3-dd87-4568-bb5b-aadd0777f1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d9f86-bd20-4ffc-b6fe-fe5417452d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
